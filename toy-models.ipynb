{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d97a7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.func import vmap, functional_call, grad\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, replace\n",
    "import numpy as np\n",
    "import einops\n",
    "from tqdm.notebook import trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f0a2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "  n_features: int\n",
    "  n_hidden: int\n",
    "  n_experts: int # total number of experts\n",
    "  n_active_experts:int  # no of active experts\n",
    "  load_balancing_loss: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b60e2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEModel(nn.Module):\n",
    "  def __init__(self, \n",
    "               config, \n",
    "               feature_probability: Optional[torch.Tensor] = None,\n",
    "               importance: Optional[torch.Tensor] = None,               \n",
    "               device='cuda'):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    self.W_experts = nn.Parameter(torch.empty((config.n_experts, config.n_features, config.n_hidden), device=device))\n",
    "    nn.init.xavier_normal_(self.W_experts)\n",
    "    self.b_final = nn.Parameter(torch.zeros((config.n_experts, config.n_features), device=device))\n",
    "    self.gate = nn.Parameter(torch.zeros((config.n_experts, config.n_features), device=device))\n",
    "\n",
    "    if feature_probability is None:\n",
    "      feature_probability = torch.ones(())\n",
    "    self.feature_probability = feature_probability.to(device)\n",
    "    if importance is None:\n",
    "      importance = torch.ones(())\n",
    "    self.importance = importance.to(device)\n",
    "\n",
    "  def compute_active_experts(self, features):   \n",
    "    # features: [..., n_features]\n",
    "    # gate: [n_experts, n_features]     \n",
    "    gate_scores = torch.einsum(\"...f,ef->...e\", features, self.gate)\n",
    "    gate_probs = F.softmax(gate_scores, dim=-1)\n",
    "    \n",
    "    top_k_values, top_k_indices = torch.topk(gate_probs, k=self.config.n_active_experts, dim=-1)\n",
    "    active_mask = torch.zeros_like(gate_probs)\n",
    "    active_mask = active_mask.scatter(-1, top_k_indices, 1.0)    \n",
    "    \n",
    "    load_balance_loss = None\n",
    "    if self.config.load_balancing_loss:\n",
    "      # P_i: average router probability for expert i (before top-k selection)\n",
    "      P_i = torch.mean(gate_probs, dim=tuple(range(gate_probs.dim() - 1)))\n",
    "      \n",
    "      # f_i: fraction of tokens actually dispatched to expert i (after top-k selection)\n",
    "      f_i = torch.mean(active_mask, dim=tuple(range(active_mask.dim() - 1)))\n",
    "      \n",
    "      N = self.config.n_experts\n",
    "      alpha = 0.01\n",
    "      load_balance_loss = alpha * N * torch.sum(f_i * P_i)\n",
    "    \n",
    "    # renormalize gating weights for active experts only\n",
    "    # sum of probabilities for active experts\n",
    "    active_sum = torch.sum(gate_probs * active_mask, dim=-1, keepdim=True)\n",
    "    \n",
    "    renormalized_weights = torch.where(\n",
    "        active_mask.bool(),\n",
    "        gate_probs / active_sum,\n",
    "        torch.zeros_like(gate_probs)\n",
    "    )\n",
    "    return renormalized_weights, top_k_indices, load_balance_loss\n",
    "\n",
    "\n",
    "  def forward(self, features):\n",
    "    # features: [..., n_features]    \n",
    "\n",
    "    expert_weights, top_k_indices, load_balance_loss = self.compute_active_experts(features)\n",
    "    \n",
    "    # hidden: [..., n_experts, n_hidden] - compression\n",
    "    hidden = torch.einsum(\"...f,efh->...eh\", features, self.W_experts)\n",
    "    \n",
    "    # expert_outputs: [..., n_experts, n_features]\n",
    "    expert_outputs = torch.einsum(\"...eh,efh->...ef\", hidden, self.W_experts)\n",
    "    expert_outputs = expert_outputs + self.b_final\n",
    "    expert_outputs = F.relu(expert_outputs)\n",
    "  \n",
    "    # final_output: [..., n_features] - recons\n",
    "    final_output = torch.einsum(\"...e,...ef->...f\", expert_weights, expert_outputs)\n",
    "    return final_output, load_balance_loss\n",
    "\n",
    "  def generate_batch(self, n_batch):\n",
    "    feat = torch.rand((n_batch, self.config.n_features), device=self.W_experts.device)\n",
    "    batch = torch.where(\n",
    "        torch.rand((n_batch, self.config.n_features), device=self.W_experts.device) <= self.feature_probability,\n",
    "        feat,\n",
    "        torch.zeros((), device=self.W_experts.device),\n",
    "    )\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bdb1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_lr(step, steps):\n",
    "  return (1 - (step / steps))\n",
    "\n",
    "def constant_lr(*_):\n",
    "  return 1.0\n",
    "\n",
    "def cosine_decay_lr(step, steps):\n",
    "  return np.cos(0.5 * np.pi * step / (steps - 1))\n",
    "\n",
    "def optimize(model, \n",
    "             render=False, \n",
    "             n_batch=1024,\n",
    "             steps=10_000,\n",
    "             print_freq=100,\n",
    "             lr=1e-3,\n",
    "             lr_scale=constant_lr,\n",
    "             hooks=[]):\n",
    "  cfg = model.config\n",
    "\n",
    "  opt = torch.optim.AdamW(list(model.parameters()), lr=lr)\n",
    "\n",
    "  start = time.time()\n",
    "  # Replace trange with regular range\n",
    "  for step in range(steps):\n",
    "    step_lr = lr * lr_scale(step, steps)\n",
    "    for group in opt.param_groups:\n",
    "      group['lr'] = step_lr\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    batch = model.generate_batch(n_batch)\n",
    "    out, load_balance_loss = model(batch)\n",
    "    error = (model.importance*(batch.abs() - out)**2)\n",
    "    reconstruction_loss = einops.reduce(error, 'b f -> f', 'mean').sum()\n",
    "    \n",
    "    loss = reconstruction_loss\n",
    "    if load_balance_loss is not None:\n",
    "      loss = loss + load_balance_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "  \n",
    "    if hooks:\n",
    "      hook_data = dict(model=model,\n",
    "                       step=step, \n",
    "                       opt=opt,\n",
    "                       error=error,\n",
    "                       loss=loss,\n",
    "                       reconstruction_loss=reconstruction_loss,\n",
    "                       load_balance_loss=load_balance_loss,\n",
    "                       lr=step_lr)\n",
    "      for h in hooks:\n",
    "        h(hook_data)\n",
    "    if step % print_freq == 0 or (step + 1 == steps):\n",
    "      print(f\"Step {step}: loss={loss.item():.6f}, lr={step_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c3de005",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  DEVICE = 'cuda'\n",
    "else:\n",
    "  DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98fc3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    n_features = 5,\n",
    "    n_hidden = 2,\n",
    "    n_experts = 10,\n",
    "    n_active_experts = 3,\n",
    "    load_balancing_loss = True,\n",
    ")\n",
    "\n",
    "model = MoEModel(\n",
    "    config=config,\n",
    "    device=DEVICE,\n",
    "    importance = 0.9**torch.arange(config.n_features),\n",
    "    feature_probability = torch.tensor(0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b41f3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa81f160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss=0.264533, lr=0.001000\n",
      "Step 5: loss=0.186584, lr=0.001000\n",
      "Step 10: loss=0.078934, lr=0.001000\n",
      "Step 15: loss=0.168268, lr=0.001000\n",
      "Step 20: loss=0.072398, lr=0.001000\n",
      "Step 25: loss=0.081440, lr=0.001000\n",
      "Step 30: loss=0.101857, lr=0.001000\n",
      "Step 35: loss=0.041873, lr=0.001000\n",
      "Step 40: loss=0.063439, lr=0.001000\n",
      "Step 45: loss=0.156674, lr=0.001000\n",
      "Step 49: loss=0.142573, lr=0.001000\n"
     ]
    }
   ],
   "source": [
    "optimize(model, n_batch=10, steps=50, print_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28db32a",
   "metadata": {},
   "source": [
    "### `vmap implementation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53bea474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_functional_model(config, device, importance, feature_probability):\n",
    "    \"\"\"separates the model's computation (functions) from its params for vmap\"\"\"\n",
    "    model = MoEModel(config, device=device, importance=importance, feature_probability=feature_probability)\n",
    "    \n",
    "    # Extract parameters and buffers as dictionaries\n",
    "    params = dict(model.named_parameters())\n",
    "    buffers = dict(model.named_buffers())\n",
    "    \n",
    "    def func_model(params_dict, buffers_dict, *inputs):\n",
    "        state_dict = {**params_dict, **buffers_dict}\n",
    "        return functional_call(model, state_dict, inputs[0] if len(inputs) == 1 else inputs)\n",
    "    \n",
    "    return func_model, params, buffers\n",
    "\n",
    "def vectorized_forward(params_batch, buffers_batch, features_batch, func_model):\n",
    "  \"\"\"in_dims tells vmap that first dim of each input is batch\"\"\"\n",
    "  return vmap(func_model, in_dims=(0, 0, 0))(params_batch, buffers_batch, features_batch)\n",
    "\n",
    "def generate_vectorized_batch(configs, feature_probs, n_batch, device):\n",
    "    batches = []\n",
    "    for config, feat_prob in zip(configs, feature_probs):\n",
    "        feat = torch.rand((n_batch, config.n_features), device=device)\n",
    "        batch = torch.where(\n",
    "            torch.rand((n_batch, config.n_features), device=device) <= feat_prob,\n",
    "            feat,\n",
    "            torch.zeros((), device=device)\n",
    "        )\n",
    "        batches.append(batch)\n",
    "    return torch.stack(batches)  # Shape: [n_models, n_batch, n_features]\n",
    "\n",
    "def stack_state_dicts(state_dicts):\n",
    "    \"\"\"stack a list of state dictionaries into a single state dict with batched tensors\"\"\"\n",
    "    if not state_dicts:\n",
    "        return {}\n",
    "    \n",
    "    stacked = {}\n",
    "    for key in state_dicts[0].keys():\n",
    "        stacked_tensor = torch.stack([sd[key] for sd in state_dicts])\n",
    "        stacked[key] = stacked_tensor.detach().requires_grad_(True)\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df484f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_vectorized(configs, feature_probs, importances, \n",
    "                        device=DEVICE,\n",
    "                        n_batch=1024, \n",
    "                        steps=10_000, \n",
    "                        print_freq=100, \n",
    "                        lr=1e-3, \n",
    "                        lr_scale=constant_lr, \n",
    "                        hooks=[]):\n",
    "    \n",
    "    func_models = []\n",
    "    all_params = []\n",
    "    all_buffers = []\n",
    "\n",
    "    for config, feat_prob, importance in zip(configs, feature_probs, importances):\n",
    "        func_model, params, buffers = make_functional_model(config, device, importance, feat_prob)\n",
    "        func_models.append(func_model)\n",
    "        all_params.append(params)\n",
    "        all_buffers.append(buffers)\n",
    "\n",
    "    stacked_params = stack_state_dicts(all_params)\n",
    "    stacked_buffers = stack_state_dicts(all_buffers)\n",
    "\n",
    "    flat_params = list(stacked_params.values())\n",
    "    \n",
    "    opt = torch.optim.AdamW(flat_params, lr=lr)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Replace trange with regular range\n",
    "    for step in range(steps):\n",
    "        step_lr = lr * lr_scale(step, steps)\n",
    "        for group in opt.param_groups:\n",
    "            group['lr'] = step_lr\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        batch = generate_vectorized_batch(configs, feature_probs, n_batch, device)\n",
    "        # Use the first func_model since they should all have the same signature\n",
    "        out, load_balance_loss = vectorized_forward(stacked_params, stacked_buffers, batch, func_models[0])\n",
    "\n",
    "        stacked_importance = torch.stack(importances)\n",
    "        error = stacked_importance.unsqueeze(1) * (batch.abs() - out)**2\n",
    "\n",
    "        reconstruction_losses = einops.reduce(error, 'models b f -> models', 'mean')\n",
    "        losses = reconstruction_losses\n",
    "        if load_balance_loss is not None:\n",
    "            losses = losses + load_balance_loss\n",
    "\n",
    "        total_loss = losses.sum()\n",
    "\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if hooks:\n",
    "            hook_data = dict(models=func_models,\n",
    "                            step=step, \n",
    "                            opt=opt,\n",
    "                            errors=error,\n",
    "                            losses=losses,\n",
    "                            total_loss=total_loss,\n",
    "                            reconstruction_losses=reconstruction_losses,\n",
    "                            load_balance_losses=load_balance_loss,\n",
    "                            lr=step_lr)\n",
    "            \n",
    "            for h in hooks:\n",
    "                h(hook_data)\n",
    "        if step % print_freq == 0 or (step + 1 == steps):\n",
    "            print(f\"Step {step}: avg_loss={losses.mean().item():.6f}, lr={step_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1db8c8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: avg_loss=0.066306, lr=0.001000\n",
      "Step 99: avg_loss=0.044675, lr=0.001000\n"
     ]
    }
   ],
   "source": [
    "configs = [\n",
    "    Config(n_features=5, n_hidden=3, n_experts=5, n_active_experts=2, load_balancing_loss=True),\n",
    "    Config(n_features=5, n_hidden=3, n_experts=5, n_active_experts=2, load_balancing_loss=False),\n",
    "]\n",
    "\n",
    "feature_probs = [torch.tensor(0.1), torch.tensor(0.2),]\n",
    "importances = [0.9**torch.arange(5), 0.8**torch.arange(5),]\n",
    "\n",
    "optimize_vectorized(configs, feature_probs, importances, n_batch=10, steps=100, device=DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
