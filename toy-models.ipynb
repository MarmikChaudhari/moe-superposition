{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d97a7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.func import vmap, functional_call, grad\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, replace\n",
    "import numpy as np\n",
    "import einops\n",
    "from tqdm.notebook import trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f0a2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "  n_features: int\n",
    "  n_hidden: int\n",
    "  n_experts: int # total number of experts\n",
    "  n_active_experts:int  # no of active experts\n",
    "  load_balancing_loss: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEModel(nn.Module):\n",
    "  def __init__(self, \n",
    "               config, \n",
    "               feature_probability: Optional[torch.Tensor] = None,\n",
    "               importance: Optional[torch.Tensor] = None,               \n",
    "               device='cuda'):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    self.W_experts = nn.Parameter(torch.empty((config.n_experts, config.n_features, config.n_hidden), device=device))\n",
    "    nn.init.xavier_normal_(self.W_experts)\n",
    "    self.b_final = nn.Parameter(torch.zeros((config.n_experts, config.n_features), device=device))\n",
    "    self.gate = nn.Parameter(torch.zeros((config.n_experts, config.n_features), device=device))\n",
    "\n",
    "    if feature_probability is None:\n",
    "      feature_probability = torch.ones(())\n",
    "    self.feature_probability = feature_probability.to(device)\n",
    "    if importance is None:\n",
    "      importance = torch.ones(())\n",
    "    self.importance = importance.to(device)\n",
    "\n",
    "  def compute_active_experts(self, features):   \n",
    "    # features: [..., n_features]\n",
    "    # gate: [n_experts, n_features]     \n",
    "    gate_scores = torch.einsum(\"...f,ef->...e\", features, self.gate)\n",
    "    gate_probs = F.softmax(gate_scores, dim=-1)\n",
    "    \n",
    "    top_k_values, top_k_indices = torch.topk(gate_probs, k=self.config.n_active_experts, dim=-1)\n",
    "    active_mask = torch.zeros_like(gate_probs)\n",
    "    active_mask = active_mask.scatter(-1, top_k_indices, 1.0)    \n",
    "    \n",
    "    load_balance_loss = None\n",
    "    if self.config.load_balancing_loss:\n",
    "      # P_i: average router probability for expert i (before top-k selection)\n",
    "      P_i = torch.mean(gate_probs, dim=tuple(range(gate_probs.dim() - 1)))\n",
    "      \n",
    "      # f_i: fraction of tokens actually dispatched to expert i (after top-k selection)\n",
    "      f_i = torch.mean(active_mask, dim=tuple(range(active_mask.dim() - 1)))\n",
    "      \n",
    "      N = self.config.n_experts\n",
    "      alpha = 0.01\n",
    "      load_balance_loss = alpha * N * torch.sum(f_i * P_i)\n",
    "    \n",
    "    # renormalize gating weights for active experts only\n",
    "    # sum of probabilities for active experts\n",
    "    active_sum = torch.sum(gate_probs * active_mask, dim=-1, keepdim=True)\n",
    "    \n",
    "    renormalized_weights = torch.where(\n",
    "        active_mask.bool(),\n",
    "        gate_probs / active_sum,\n",
    "        torch.zeros_like(gate_probs)\n",
    "    )\n",
    "    return renormalized_weights, top_k_indices, load_balance_loss\n",
    "\n",
    "\n",
    "  def forward(self, features):\n",
    "    # features: [..., n_features]    \n",
    "\n",
    "    expert_weights, top_k_indices, load_balance_loss = self.compute_active_experts(features)\n",
    "    \n",
    "    # hidden: [..., n_experts, n_hidden] - compression\n",
    "    hidden = torch.einsum(\"...f,efh->...eh\", features, self.W_experts)\n",
    "    \n",
    "    # expert_outputs: [..., n_experts, n_features]\n",
    "    expert_outputs = torch.einsum(\"...eh,efh->...ef\", hidden, self.W_experts)\n",
    "    expert_outputs = expert_outputs + self.b_final\n",
    "    expert_outputs = F.relu(expert_outputs)\n",
    "  \n",
    "    # final_output: [..., n_features] - recons\n",
    "    final_output = torch.einsum(\"...e,...ef->...f\", expert_weights, expert_outputs)\n",
    "    return final_output, load_balance_loss\n",
    "\n",
    "  def generate_batch(self, n_batch):\n",
    "    feat = torch.rand((n_batch, self.config.n_features), device=self.W_experts.device)\n",
    "    batch = torch.where(\n",
    "        torch.rand((n_batch, self.config.n_features), device=self.W_experts.device) <= self.feature_probability,\n",
    "        feat,\n",
    "        torch.zeros((), device=self.W_experts.device),\n",
    "    )\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bdb1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_lr(step, steps):\n",
    "  return (1 - (step / steps))\n",
    "\n",
    "def constant_lr(*_):\n",
    "  return 1.0\n",
    "\n",
    "def cosine_decay_lr(step, steps):\n",
    "  return np.cos(0.5 * np.pi * step / (steps - 1))\n",
    "\n",
    "def optimize(model, \n",
    "             render=False, \n",
    "             n_batch=1024,\n",
    "             steps=10_000,\n",
    "             print_freq=100,\n",
    "             lr=1e-3,\n",
    "             lr_scale=constant_lr,\n",
    "             hooks=[]):\n",
    "  cfg = model.config\n",
    "\n",
    "  opt = torch.optim.AdamW(list(model.parameters()), lr=lr)\n",
    "\n",
    "  start = time.time()\n",
    "  # Replace trange with regular range\n",
    "  for step in range(steps):\n",
    "    step_lr = lr * lr_scale(step, steps)\n",
    "    for group in opt.param_groups:\n",
    "      group['lr'] = step_lr\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    batch = model.generate_batch(n_batch)\n",
    "    out, load_balance_loss = model(batch)\n",
    "    error = (model.importance*(batch.abs() - out)**2)\n",
    "    reconstruction_loss = einops.reduce(error, 'b f -> f', 'mean').sum()\n",
    "    \n",
    "    loss = reconstruction_loss\n",
    "    if load_balance_loss is not None:\n",
    "      loss = loss + load_balance_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "  \n",
    "    if hooks:\n",
    "      hook_data = dict(model=model,\n",
    "                       step=step, \n",
    "                       opt=opt,\n",
    "                       error=error,\n",
    "                       loss=loss,\n",
    "                       reconstruction_loss=reconstruction_loss,\n",
    "                       load_balance_loss=load_balance_loss,\n",
    "                       lr=step_lr)\n",
    "      for h in hooks:\n",
    "        h(hook_data)\n",
    "    if step % print_freq == 0 or (step + 1 == steps):\n",
    "      print(f\"Step {step}: loss={loss.item():.6f}, lr={step_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c3de005",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  DEVICE = 'cuda'\n",
    "else:\n",
    "  DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98fc3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    n_features = 5,\n",
    "    n_hidden = 2,\n",
    "    n_experts = 10,\n",
    "    n_active_experts = 3,\n",
    "    load_balancing_loss = True,\n",
    ")\n",
    "\n",
    "model = MoEModel(\n",
    "    config=config,\n",
    "    device=DEVICE,\n",
    "    importance = 0.9**torch.arange(config.n_features),\n",
    "    feature_probability = torch.tensor(0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b41f3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa81f160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss=30.146425, lr=0.001000\n",
      "Step 5: loss=30.218515, lr=0.001000\n",
      "Step 10: loss=30.116838, lr=0.001000\n",
      "Step 15: loss=30.115690, lr=0.001000\n",
      "Step 20: loss=30.036596, lr=0.001000\n",
      "Step 25: loss=30.015856, lr=0.001000\n",
      "Step 30: loss=30.022730, lr=0.001000\n",
      "Step 35: loss=30.022266, lr=0.001000\n",
      "Step 40: loss=29.982704, lr=0.001000\n",
      "Step 45: loss=29.984022, lr=0.001000\n",
      "Step 49: loss=29.936220, lr=0.001000\n"
     ]
    }
   ],
   "source": [
    "optimize(model, n_batch=10, steps=50, print_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28db32a",
   "metadata": {},
   "source": [
    "### `vmap implementation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53bea474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_functional_model(config, device, importance, feature_probability):\n",
    "    \"\"\"separates the model's computation (functions) from its params for vmap\"\"\"\n",
    "    model = MoEModel(config, device=device, importance=importance, feature_probability=feature_probability)\n",
    "    \n",
    "    # Extract parameters and buffers as dictionaries\n",
    "    params = dict(model.named_parameters())\n",
    "    buffers = dict(model.named_buffers())\n",
    "    \n",
    "    def func_model(params_dict, buffers_dict, *inputs):\n",
    "        state_dict = {**params_dict, **buffers_dict}\n",
    "        return functional_call(model, state_dict, inputs[0] if len(inputs) == 1 else inputs)\n",
    "    \n",
    "    return func_model, params, buffers\n",
    "\n",
    "def vectorized_forward(params_batch, buffers_batch, features_batch, func_model):\n",
    "  \"\"\"in_dims tells vmap that first dim of each input is batch\"\"\"\n",
    "  return vmap(func_model, in_dims=(0, 0, 0))(params_batch, buffers_batch, features_batch)\n",
    "\n",
    "def generate_vectorized_batch(configs, feature_probs, n_batch, device):\n",
    "    batches = []\n",
    "    for config, feat_prob in zip(configs, feature_probs):\n",
    "        feat = torch.rand((n_batch, config.n_features), device=device)\n",
    "        batch = torch.where(\n",
    "            torch.rand((n_batch, config.n_features), device=device) <= feat_prob,\n",
    "            feat,\n",
    "            torch.zeros((), device=device)\n",
    "        )\n",
    "        batches.append(batch)\n",
    "    return torch.stack(batches)  # Shape: [n_models, n_batch, n_features]\n",
    "\n",
    "def stack_state_dicts(state_dicts):\n",
    "    \"\"\"stack a list of state dictionaries into a single state dict with batched tensors\"\"\"\n",
    "    if not state_dicts:\n",
    "        return {}\n",
    "    \n",
    "    stacked = {}\n",
    "    for key in state_dicts[0].keys():\n",
    "        stacked_tensor = torch.stack([sd[key] for sd in state_dicts])\n",
    "        stacked[key] = stacked_tensor.detach().requires_grad_(True)\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df484f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_vectorized(configs, feature_probs, importances, \n",
    "                        device=DEVICE,\n",
    "                        n_batch=1024, \n",
    "                        steps=10_000, \n",
    "                        print_freq=100, \n",
    "                        lr=1e-3, \n",
    "                        lr_scale=constant_lr, \n",
    "                        hooks=[]):\n",
    "    \n",
    "    func_models = []\n",
    "    all_params = []\n",
    "    all_buffers = []\n",
    "\n",
    "    for config, feat_prob, importance in zip(configs, feature_probs, importances):\n",
    "        func_model, params, buffers = make_functional_model(config, device, importance, feat_prob)\n",
    "        func_models.append(func_model)\n",
    "        all_params.append(params)\n",
    "        all_buffers.append(buffers)\n",
    "\n",
    "    stacked_params = stack_state_dicts(all_params)\n",
    "    stacked_buffers = stack_state_dicts(all_buffers)\n",
    "\n",
    "    flat_params = list(stacked_params.values())\n",
    "    \n",
    "    opt = torch.optim.AdamW(flat_params, lr=lr)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Replace trange with regular range\n",
    "    for step in range(steps):\n",
    "        step_lr = lr * lr_scale(step, steps)\n",
    "        for group in opt.param_groups:\n",
    "            group['lr'] = step_lr\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        batch = generate_vectorized_batch(configs, feature_probs, n_batch, device)\n",
    "        # Use the first func_model since they should all have the same signature\n",
    "        out, load_balance_loss = vectorized_forward(stacked_params, stacked_buffers, batch, func_models[0])\n",
    "\n",
    "        stacked_importance = torch.stack(importances)\n",
    "        error = stacked_importance.unsqueeze(1) * (batch.abs() - out)**2\n",
    "\n",
    "        reconstruction_losses = einops.reduce(error, 'models b f -> models', 'mean')\n",
    "        losses = reconstruction_losses\n",
    "        if load_balance_loss is not None:\n",
    "            losses = losses + load_balance_loss\n",
    "\n",
    "        total_loss = losses.sum()\n",
    "\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if hooks:\n",
    "            hook_data = dict(models=func_models,\n",
    "                            step=step, \n",
    "                            opt=opt,\n",
    "                            errors=error,\n",
    "                            losses=losses,\n",
    "                            total_loss=total_loss,\n",
    "                            reconstruction_losses=reconstruction_losses,\n",
    "                            load_balance_losses=load_balance_loss,\n",
    "                            lr=step_lr)\n",
    "            \n",
    "            for h in hooks:\n",
    "                h(hook_data)\n",
    "        if step % print_freq == 0 or (step + 1 == steps):\n",
    "            print(f\"Step {step}: avg_loss={losses.mean().item():.6f}, lr={step_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1db8c8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: avg_loss=20.020626, lr=0.001000\n",
      "Step 99: avg_loss=19.989468, lr=0.001000\n"
     ]
    }
   ],
   "source": [
    "configs = [\n",
    "    Config(n_features=5, n_hidden=3, n_experts=5, n_active_experts=2, load_balancing_loss=True),\n",
    "    Config(n_features=5, n_hidden=3, n_experts=5, n_active_experts=2, load_balancing_loss=False),\n",
    "]\n",
    "\n",
    "feature_probs = [torch.tensor(0.1), torch.tensor(0.2),]\n",
    "importances = [0.9**torch.arange(5), 0.8**torch.arange(5),]\n",
    "\n",
    "optimize_vectorized(configs, feature_probs, importances, n_batch=10, steps=100, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495cbf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Expert Weight Classifications ===\n",
      "\n",
      "expert_0:\n",
      "  Classification: incomprehensible\n",
      "  Raw weights: [0.2856999933719635, 0.3747999966144562]\n",
      "  Description: roughly_equal_magnitude\n",
      "  Mean magnitude: 0.701\n",
      "\n",
      "expert_1:\n",
      "  Classification: superposition\n",
      "  Raw weights: [-0.9991000294685364, 0.44020000100135803]\n",
      "  Pattern: superposition_neg1_1\n",
      "  Similarity: 0.932\n"
     ]
    }
   ],
   "source": [
    "# Tools to classify expert weights into different types (superposition, orthogonal, incomprehensible)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "def classify_expert_weights(expert_weights: torch.Tensor, tolerance: float = 0.1) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Classify expert weight matrices into different types.\n",
    "    \n",
    "    Args:\n",
    "        expert_weights: Shape [n_experts, n_features, n_hidden]\n",
    "        tolerance: Tolerance for considering values as 0, 1, or -1\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with classification results\n",
    "    \"\"\"\n",
    "    n_experts, n_features, n_hidden = expert_weights.shape\n",
    "    classifications = {}\n",
    "    \n",
    "    for expert_id in range(n_experts):\n",
    "        weights = expert_weights[expert_id].squeeze()  # Shape: [n_features]\n",
    "        \n",
    "        # Normalize weights to unit norm for comparison\n",
    "        weights_norm = weights / torch.norm(weights)\n",
    "        \n",
    "        # Check for superposition patterns\n",
    "        is_superposition = classify_superposition(weights_norm, tolerance)\n",
    "        \n",
    "        # Check for orthogonal feature patterns\n",
    "        is_orthogonal = classify_orthogonal(weights_norm, tolerance)\n",
    "        \n",
    "        # Check for incomprehensible patterns\n",
    "        is_incomprehensible = classify_incomprehensible(weights_norm, tolerance)\n",
    "        \n",
    "        # Determine primary classification\n",
    "        if is_superposition['is_superposition']:\n",
    "            classification = 'superposition'\n",
    "            details = is_superposition\n",
    "        elif is_orthogonal['is_orthogonal']:\n",
    "            classification = 'orthogonal'\n",
    "            details = is_orthogonal\n",
    "        elif is_incomprehensible['is_incomprehensible']:\n",
    "            classification = 'incomprehensible'\n",
    "            details = is_incomprehensible\n",
    "        else:\n",
    "            classification = 'other'\n",
    "            details = {'weights': weights_norm.tolist()}\n",
    "        \n",
    "        classifications[f'expert_{expert_id}'] = {\n",
    "            'classification': classification,\n",
    "            'details': details,\n",
    "            'raw_weights': weights.tolist()\n",
    "        }\n",
    "    \n",
    "    return classifications\n",
    "\n",
    "def classify_superposition(weights: torch.Tensor, tolerance: float) -> Dict[str, Any]:\n",
    "    \"\"\"Check if weights match superposition patterns like (1, -1) or (-1, 1).\"\"\"\n",
    "    weights_np = weights.cpu().numpy()\n",
    "    \n",
    "    # Check for (1, -1) pattern\n",
    "    pattern1 = np.array([1.0, -1.0])\n",
    "    pattern1_norm = pattern1 / np.linalg.norm(pattern1)\n",
    "    \n",
    "    # Check for (-1, 1) pattern\n",
    "    pattern2 = np.array([-1.0, 1.0])\n",
    "    pattern2_norm = pattern2 / np.linalg.norm(pattern2)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarity1 = np.abs(np.dot(weights_np, pattern1_norm))\n",
    "    similarity2 = np.abs(np.dot(weights_np, pattern2_norm))\n",
    "    \n",
    "    max_similarity = max(similarity1, similarity2)\n",
    "    \n",
    "    if max_similarity > (1.0 - tolerance):\n",
    "        if similarity1 > similarity2:\n",
    "            pattern = 'superposition_1_neg1'\n",
    "            pattern_weights = pattern1_norm\n",
    "        else:\n",
    "            pattern = 'superposition_neg1_1'\n",
    "            pattern_weights = pattern2_norm\n",
    "        \n",
    "        return {\n",
    "            'is_superposition': True,\n",
    "            'pattern': pattern,\n",
    "            'similarity': max_similarity,\n",
    "            'pattern_weights': pattern_weights.tolist()\n",
    "        }\n",
    "    \n",
    "    return {'is_superposition': False}\n",
    "\n",
    "def classify_orthogonal(weights: torch.Tensor, tolerance: float) -> Dict[str, Any]:\n",
    "    \"\"\"Check if weights match orthogonal patterns like (1, 0) or (0, 1).\"\"\"\n",
    "    weights_np = weights.cpu().numpy()\n",
    "    \n",
    "    # Check for (1, 0) pattern\n",
    "    pattern1 = np.array([1.0, 0.0])\n",
    "    pattern1_norm = pattern1 / np.linalg.norm(pattern1)\n",
    "    \n",
    "    # Check for (0, 1) pattern\n",
    "    pattern2 = np.array([0.0, 1.0])\n",
    "    pattern2_norm = pattern2 / np.linalg.norm(pattern2)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarity1 = np.abs(np.dot(weights_np, pattern1_norm))\n",
    "    similarity2 = np.abs(np.dot(weights_np, pattern2_norm))\n",
    "    \n",
    "    max_similarity = max(similarity1, similarity2)\n",
    "    \n",
    "    if max_similarity > (1.0 - tolerance):\n",
    "        if similarity1 > similarity2:\n",
    "            pattern = 'orthogonal_feature_0'\n",
    "            pattern_weights = pattern1_norm\n",
    "        else:\n",
    "            pattern = 'orthogonal_feature_1'\n",
    "            pattern_weights = pattern2_norm\n",
    "        \n",
    "        return {\n",
    "            'is_orthogonal': True,\n",
    "            'pattern': pattern,\n",
    "            'similarity': max_similarity,\n",
    "            'pattern_weights': pattern_weights.tolist()\n",
    "        }\n",
    "    \n",
    "    return {'is_orthogonal': False}\n",
    "\n",
    "def classify_incomprehensible(weights: torch.Tensor, tolerance: float) -> Dict[str, Any]:\n",
    "    \"\"\"Check if weights don't match any clear pattern.\"\"\"\n",
    "    weights_np = weights.cpu().numpy()\n",
    "    \n",
    "    # Check if weights are roughly equal magnitude but not clear patterns\n",
    "    weights_abs = np.abs(weights_np)\n",
    "    mean_magnitude = np.mean(weights_abs)\n",
    "    std_magnitude = np.std(weights_abs)\n",
    "    \n",
    "    # If weights are roughly equal magnitude but not clear patterns\n",
    "    if std_magnitude < tolerance and mean_magnitude > 0.1:\n",
    "        return {\n",
    "            'is_incomprehensible': True,\n",
    "            'mean_magnitude': mean_magnitude,\n",
    "            'std_magnitude': std_magnitude,\n",
    "            'description': 'roughly_equal_magnitude'\n",
    "        }\n",
    "    \n",
    "    # If weights are very small\n",
    "    if mean_magnitude < tolerance:\n",
    "        return {\n",
    "            'is_incomprehensible': True,\n",
    "            'mean_magnitude': mean_magnitude,\n",
    "            'description': 'very_small_weights'\n",
    "        }\n",
    "    \n",
    "    return {'is_incomprehensible': False}\n",
    "\n",
    "def print_classification_summary(classifications: Dict[str, Any]):\n",
    "    \"\"\"Print a summary of expert classifications.\"\"\"\n",
    "    print(\"=== Expert Weight Classifications ===\")\n",
    "    \n",
    "    for expert_id, result in classifications.items():\n",
    "        classification = result['classification']\n",
    "        details = result['details']\n",
    "        \n",
    "        print(f\"\\n{expert_id}:\")\n",
    "        print(f\"  Classification: {classification}\")\n",
    "        print(f\"  Raw weights: {result['raw_weights']}\")\n",
    "        \n",
    "        if classification == 'superposition':\n",
    "            print(f\"  Pattern: {details['pattern']}\")\n",
    "            print(f\"  Similarity: {details['similarity']:.3f}\")\n",
    "        elif classification == 'orthogonal':\n",
    "            print(f\"  Pattern: {details['pattern']}\")\n",
    "            print(f\"  Similarity: {details['similarity']:.3f}\")\n",
    "        elif classification == 'incomprehensible':\n",
    "            print(f\"  Description: {details['description']}\")\n",
    "            print(f\"  Mean magnitude: {details['mean_magnitude']:.3f}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with your expert weights\n",
    "    expert_weights = torch.tensor([[[ 0.2857],\n",
    "                                   [ 0.3748]],\n",
    "                                  [[-0.9991],\n",
    "                                   [ 0.4402]]])\n",
    "    \n",
    "    classifications = classify_expert_weights(expert_weights, tolerance=0.1)\n",
    "    print_classification_summary(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab42d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Step 0: loss=10.005443, lr=0.001000\n",
      "Step 500: loss=9.961046, lr=0.001000\n",
      "Step 1000: loss=9.930203, lr=0.001000\n",
      "Step 1500: loss=9.917327, lr=0.001000\n",
      "Step 2000: loss=9.836444, lr=0.001000\n",
      "Step 2500: loss=9.851954, lr=0.001000\n",
      "Step 3000: loss=9.774613, lr=0.001000\n",
      "Step 3500: loss=9.799587, lr=0.001000\n",
      "Step 4000: loss=9.797658, lr=0.001000\n",
      "Step 4500: loss=9.785119, lr=0.001000\n",
      "Step 4999: loss=9.827586, lr=0.001000\n",
      "Gate matrix:\n",
      "Parameter containing:\n",
      "tensor([[-3.0730, -3.0572, -3.0687],\n",
      "        [ 3.0730,  3.0572,  3.0687]], requires_grad=True)\n",
      "Expert weights:\n",
      "Parameter containing:\n",
      "tensor([[[-0.0955,  0.4050],\n",
      "         [ 0.1420, -0.0509],\n",
      "         [-0.4971, -0.2047]],\n",
      "\n",
      "        [[-0.7263, -0.6883],\n",
      "         [ 0.7235, -0.6893],\n",
      "         [ 0.6434,  0.7625]]], requires_grad=True)\n",
      "\n",
      "Final model parameters:\n",
      "Feature probability: 0.009999999776482582\n",
      "Importance weights: tensor([1, 1, 1])\n",
      "\n",
      "Expert Weight Classifications:\n",
      "\n",
      "expert_0:\n",
      "  Hidden dim 0: orthogonal_feature_2 (similarity: 0.946)\n",
      "    Weights: [-0.09545814990997314, 0.14195403456687927, -0.4970741868019104]\n",
      "    Pattern: [0.0, 0.0, 1.0]\n",
      "  Hidden dim 1: superposition_pair_feature_0_feature_2_mixed (similarity: 0.944)\n",
      "    Weights: [0.40496212244033813, -0.05092032626271248, -0.20465804636478424]\n",
      "    Pattern: [1.0, 0.0, -1.0]\n",
      "\n",
      "expert_1:\n",
      "  Hidden dim 0: superposition_all_three_mixed (similarity: 0.998)\n",
      "    Weights: [-0.7263224720954895, 0.723491370677948, 0.6433688998222351]\n",
      "    Pattern: [-1.0, 1.0, 1.0]\n",
      "  Hidden dim 1: superposition_all_three_mixed (similarity: 0.999)\n",
      "    Weights: [-0.6883119344711304, -0.6893332004547119, 0.7625331878662109]\n",
      "    Pattern: [1.0, 1.0, -1.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sys.path.append('.')\n",
    "\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "  DEVICE = 'cuda'\n",
    "else:\n",
    "  DEVICE = 'cpu'\n",
    "\n",
    "config = Config(\n",
    "    n_features = 3,\n",
    "    n_hidden = 2,\n",
    "    n_experts = 2,\n",
    "    n_active_experts = 1,\n",
    "    load_balancing_loss = True,\n",
    ")\n",
    "\n",
    "# Configure importance and feature probability (sparsity)\n",
    "model = MoEModel(\n",
    "    config=config,\n",
    "    device=DEVICE,\n",
    "    importance = torch.tensor([1, 1, 1]),\n",
    "    feature_probability = torch.tensor(0.01)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "optimize(model, n_batch=512, steps=5000, print_freq=500, lr=1e-3)\n",
    "\n",
    "print(\"Gate matrix:\")\n",
    "print(model.gate)\n",
    "print(\"Expert weights:\")\n",
    "print(model.W_experts)\n",
    "\n",
    "# Print final model parameters\n",
    "print(\"\\nFinal model parameters:\")\n",
    "print(f\"Feature probability: {model.feature_probability.item()}\")\n",
    "print(f\"Importance weights: {model.importance}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3956c3b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript f has size 3 for operand 1 which does not broadcast with previously seen size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_active_experts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mMoEModel.compute_active_experts\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_active_experts\u001b[39m(\u001b[38;5;28mself\u001b[39m, features):   \n\u001b[32m     22\u001b[39m   \u001b[38;5;66;03m# features: [..., n_features]\u001b[39;00m\n\u001b[32m     23\u001b[39m   \u001b[38;5;66;03m# gate: [n_experts, n_features]     \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m   gate_scores = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m...f,ef->...e\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m   gate_probs = F.softmax(gate_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     27\u001b[39m   top_k_values, top_k_indices = torch.topk(gate_probs, k=\u001b[38;5;28mself\u001b[39m.config.n_active_experts, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/py311_env/lib/python3.11/site-packages/torch/functional.py:380\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, *_operands)\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum.enabled:\n\u001b[32m    378\u001b[39m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[32m    379\u001b[39m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    382\u001b[39m path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum.is_available():\n",
      "\u001b[31mRuntimeError\u001b[39m: einsum(): subscript f has size 3 for operand 1 which does not broadcast with previously seen size 2"
     ]
    }
   ],
   "source": [
    "model.compute_active_experts(torch.tensor([1., 0.01])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe1eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate matrix:\n",
      "Parameter containing:\n",
      "tensor([[-3.2315, -3.2264],\n",
      "        [ 3.2315,  3.2264]], requires_grad=True)\n",
      "\n",
      "Feature probability: 0.10000000149011612\n",
      "\n",
      "Router probabilities for first expert:\n",
      "With data sparsity: 0.423\n",
      "Without data sparsity (uniform): 0.020\n",
      "\n",
      "Router probabilities for all experts:\n",
      "Expert 0: with_data=0.425, without_data=0.019\n",
      "Expert 1: with_data=0.575, without_data=0.981\n"
     ]
    }
   ],
   "source": [
    "def compute_router_probabilities(gate_matrix, feature_probability=None, n_samples=10000):\n",
    "    \"\"\"\n",
    "    Compute probability that router picks first expert.\n",
    "    \n",
    "    Args:\n",
    "        gate_matrix: Shape [n_experts, n_features] - the gate weights\n",
    "        feature_probability: Probability of feature being active (None for uniform)\n",
    "        n_samples: Number of samples to estimate probability\n",
    "    \n",
    "    Returns:\n",
    "        prob_with_data: Probability considering data sparsity\n",
    "        prob_without_data: Probability assuming uniform data\n",
    "    \"\"\"\n",
    "    device = gate_matrix.device\n",
    "    n_experts, n_features = gate_matrix.shape\n",
    "    \n",
    "    # 1. Probability WITHOUT considering data properties (uniform data)\n",
    "    # Generate uniform random features\n",
    "    uniform_features = torch.rand(n_samples, n_features, device=device)\n",
    "    \n",
    "    # Compute gate scores and probabilities\n",
    "    gate_scores_uniform = torch.einsum(\"bf,ef->be\", uniform_features, gate_matrix)\n",
    "    gate_probs_uniform = F.softmax(gate_scores_uniform, dim=-1)\n",
    "    \n",
    "    # Probability of selecting first expert\n",
    "    prob_without_data = gate_probs_uniform[:, 0].mean().item()\n",
    "    \n",
    "    # 2. Probability WITH considering data properties (sparse data)\n",
    "    if feature_probability is not None:\n",
    "        # Generate sparse data according to feature_probability\n",
    "        sparse_features = torch.where(\n",
    "            torch.rand(n_samples, n_features, device=device) <= feature_probability,\n",
    "            torch.rand(n_samples, n_features, device=device),\n",
    "            torch.zeros(n_samples, n_features, device=device)\n",
    "        )\n",
    "        \n",
    "        # Compute gate scores and probabilities for sparse data\n",
    "        gate_scores_sparse = torch.einsum(\"bf,ef->be\", sparse_features, gate_matrix)\n",
    "        gate_probs_sparse = F.softmax(gate_scores_sparse, dim=-1)\n",
    "        \n",
    "        # Probability of selecting first expert\n",
    "        prob_with_data = gate_probs_sparse[:, 0].mean().item()\n",
    "    else:\n",
    "        prob_with_data = prob_without_data\n",
    "    \n",
    "    return prob_with_data, prob_without_data\n",
    "\n",
    "# Example usage:\n",
    "print(\"Gate matrix:\")\n",
    "print(model.gate)\n",
    "\n",
    "print(f\"\\nFeature probability: {model.feature_probability.item()}\")\n",
    "\n",
    "# Compute probabilities\n",
    "prob_with_data, prob_without_data = compute_router_probabilities(\n",
    "    model.gate, \n",
    "    feature_probability=model.feature_probability,\n",
    "    n_samples=10000\n",
    ")\n",
    "\n",
    "print(f\"\\nRouter probabilities for first expert:\")\n",
    "print(f\"With data sparsity: {prob_with_data:.3f}\")\n",
    "print(f\"Without data sparsity (uniform): {prob_without_data:.3f}\")\n",
    "\n",
    "# For multiple experts, you can also compute for each expert:\n",
    "def compute_all_expert_probabilities(gate_matrix, feature_probability=None, n_samples=10000):\n",
    "    \"\"\"\n",
    "    Compute probability for each expert being selected.\n",
    "    \"\"\"\n",
    "    device = gate_matrix.device\n",
    "    n_experts, n_features = gate_matrix.shape\n",
    "    \n",
    "    # Uniform data\n",
    "    uniform_features = torch.rand(n_samples, n_features, device=device)\n",
    "    gate_scores_uniform = torch.einsum(\"bf,ef->be\", uniform_features, gate_matrix)\n",
    "    gate_probs_uniform = F.softmax(gate_scores_uniform, dim=-1)\n",
    "    probs_without_data = gate_probs_uniform.mean(dim=0).cpu().detach().numpy()\n",
    "    \n",
    "    # Sparse data\n",
    "    if feature_probability is not None:\n",
    "        sparse_features = torch.where(\n",
    "            torch.rand(n_samples, n_features, device=device) <= feature_probability,\n",
    "            torch.rand(n_samples, n_features, device=device),\n",
    "            torch.zeros(n_samples, n_features, device=device)\n",
    "        )\n",
    "        gate_scores_sparse = torch.einsum(\"bf,ef->be\", sparse_features, gate_matrix)\n",
    "        gate_probs_sparse = F.softmax(gate_scores_sparse, dim=-1)\n",
    "        probs_with_data = gate_probs_sparse.mean(dim=0).cpu().detach().numpy()\n",
    "    else:\n",
    "        probs_with_data = probs_without_data\n",
    "    \n",
    "    return probs_with_data, probs_without_data\n",
    "\n",
    "# Compute for all experts\n",
    "probs_with_data, probs_without_data = compute_all_expert_probabilities(\n",
    "    model.gate, \n",
    "    feature_probability=model.feature_probability\n",
    ")\n",
    "\n",
    "print(f\"\\nRouter probabilities for all experts:\")\n",
    "for i in range(len(probs_with_data)):\n",
    "    print(f\"Expert {i}: with_data={probs_with_data[i]:.3f}, without_data={probs_without_data[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify expert weights into different types (superposition, orthogonal, incomprehensible)\n",
    "# This is a simple classification based on the weights of the expert.\n",
    "\n",
    "\n",
    "# Classify expert weights\n",
    "def classify_expert_weights(expert_weights: torch.Tensor, tolerance: float = 0.1):\n",
    "    \"\"\"Classify expert weight matrices into different types.\"\"\"\n",
    "    n_experts, n_features, n_hidden = expert_weights.shape\n",
    "    classifications = {}\n",
    "    \n",
    "    for expert_id in range(n_experts):\n",
    "        weights = expert_weights[expert_id].squeeze()  # Shape: [n_features, n_hidden]\n",
    "        \n",
    "        # For each hidden dimension, classify the feature weights\n",
    "        hidden_classifications = []\n",
    "        for hidden_dim in range(n_hidden):\n",
    "            feature_weights = weights[:, hidden_dim]  # Shape: [n_features]\n",
    "            weights_norm = feature_weights / torch.norm(feature_weights)\n",
    "            weights_np = weights_norm.cpu().detach().numpy()\n",
    "            \n",
    "            # Check for superposition patterns\n",
    "            # All three features superimposed\n",
    "            all_three_patterns = [\n",
    "                np.array([1.0, 1.0, 1.0]),\n",
    "                np.array([1.0, 1.0, -1.0]),\n",
    "                np.array([1.0, -1.0, 1.0]),\n",
    "                np.array([-1.0, 1.0, 1.0]),\n",
    "                np.array([1.0, -1.0, -1.0]),\n",
    "                np.array([-1.0, 1.0, -1.0]),\n",
    "                np.array([-1.0, -1.0, 1.0]),\n",
    "                np.array([-1.0, -1.0, -1.0])\n",
    "            ]\n",
    "            \n",
    "            # Pairs of features superimposed\n",
    "            pair_patterns = [\n",
    "                np.array([1.0, 1.0, 0.0]),  # Features 0,1 superimposed\n",
    "                np.array([1.0, 0.0, 1.0]),  # Features 0,2 superimposed\n",
    "                np.array([0.0, 1.0, 1.0]),  # Features 1,2 superimposed\n",
    "                np.array([1.0, -1.0, 0.0]), # Features 0,1 superimposed (opposite)\n",
    "                np.array([1.0, 0.0, -1.0]), # Features 0,2 superimposed (opposite)\n",
    "                np.array([0.0, 1.0, -1.0]), # Features 1,2 superimposed (opposite)\n",
    "                np.array([-1.0, 1.0, 0.0]), # Features 0,1 superimposed (opposite)\n",
    "                np.array([-1.0, 0.0, 1.0]), # Features 0,2 superimposed (opposite)\n",
    "                np.array([0.0, -1.0, 1.0])  # Features 1,2 superimposed (opposite)\n",
    "            ]\n",
    "            \n",
    "            # Single features (orthogonal)\n",
    "            single_patterns = [\n",
    "                np.array([1.0, 0.0, 0.0]),  # Feature 0 only\n",
    "                np.array([0.0, 1.0, 0.0]),  # Feature 1 only\n",
    "                np.array([0.0, 0.0, 1.0])   # Feature 2 only\n",
    "            ]\n",
    "            \n",
    "            # Test all patterns\n",
    "            max_similarity = 0\n",
    "            best_pattern = None\n",
    "            best_pattern_type = None\n",
    "            \n",
    "            # Test all-three patterns\n",
    "            for pattern in all_three_patterns:\n",
    "                pattern_norm = pattern / np.linalg.norm(pattern)\n",
    "                similarity = np.abs(np.dot(weights_np, pattern_norm))\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_pattern = pattern\n",
    "                    best_pattern_type = 'all_three'\n",
    "            \n",
    "            # Test pair patterns\n",
    "            for pattern in pair_patterns:\n",
    "                pattern_norm = pattern / np.linalg.norm(pattern)\n",
    "                similarity = np.abs(np.dot(weights_np, pattern_norm))\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_pattern = pattern\n",
    "                    best_pattern_type = 'pair'\n",
    "            \n",
    "            # Test single patterns\n",
    "            for pattern in single_patterns:\n",
    "                pattern_norm = pattern / np.linalg.norm(pattern)\n",
    "                similarity = np.abs(np.dot(weights_np, pattern_norm))\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_pattern = pattern\n",
    "                    best_pattern_type = 'single'\n",
    "            \n",
    "            # Determine classification\n",
    "            if max_similarity > (1.0 - tolerance):\n",
    "                if best_pattern_type == 'all_three':\n",
    "                    # Identify which features are superimposed\n",
    "                    active_features = np.where(np.abs(best_pattern) > 0.1)[0]\n",
    "                    signs = best_pattern[active_features]\n",
    "                    if np.all(signs > 0):\n",
    "                        classification = f'superposition_all_three_positive'\n",
    "                    elif np.all(signs < 0):\n",
    "                        classification = f'superposition_all_three_negative'\n",
    "                    else:\n",
    "                        classification = f'superposition_all_three_mixed'\n",
    "                elif best_pattern_type == 'pair':\n",
    "                    # Identify which pair is superimposed\n",
    "                    active_features = np.where(np.abs(best_pattern) > 0.1)[0]\n",
    "                    signs = best_pattern[active_features]\n",
    "                    feature_names = [f'feature_{i}' for i in active_features]\n",
    "                    if np.all(signs > 0):\n",
    "                        classification = f'superposition_pair_{\"_\".join(feature_names)}_positive'\n",
    "                    elif np.all(signs < 0):\n",
    "                        classification = f'superposition_pair_{\"_\".join(feature_names)}_negative'\n",
    "                    else:\n",
    "                        classification = f'superposition_pair_{\"_\".join(feature_names)}_mixed'\n",
    "                elif best_pattern_type == 'single':\n",
    "                    # Identify which single feature\n",
    "                    active_feature = np.where(np.abs(best_pattern) > 0.1)[0][0]\n",
    "                    classification = f'orthogonal_feature_{active_feature}'\n",
    "            else:\n",
    "                classification = 'incomprehensible'\n",
    "            \n",
    "            hidden_classifications.append({\n",
    "                'classification': classification,\n",
    "                'similarity': max_similarity,\n",
    "                'weights': feature_weights.tolist(),\n",
    "                'pattern': best_pattern.tolist() if best_pattern is not None else None\n",
    "            })\n",
    "        \n",
    "        classifications[f'expert_{expert_id}'] = {\n",
    "            'hidden_dimensions': hidden_classifications,\n",
    "            'raw_weights': weights.tolist()\n",
    "        }\n",
    "    \n",
    "    return classifications\n",
    "\n",
    "# Classify and print results\n",
    "classifications = classify_expert_weights(model.W_experts)\n",
    "print(\"\\nExpert Weight Classifications:\")\n",
    "for expert_id, result in classifications.items():\n",
    "    print(f\"\\n{expert_id}:\")\n",
    "    for i, hidden_result in enumerate(result['hidden_dimensions']):\n",
    "        print(f\"  Hidden dim {i}: {hidden_result['classification']} (similarity: {hidden_result['similarity']:.3f})\")\n",
    "        print(f\"    Weights: {hidden_result['weights']}\")\n",
    "        if hidden_result['pattern']:\n",
    "            print(f\"    Pattern: {hidden_result['pattern']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e7c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss=10.057854, lr=0.001000\n",
      "Step 100: loss=9.976372, lr=0.001000\n",
      "Step 200: loss=9.908997, lr=0.001000\n",
      "Step 300: loss=9.855118, lr=0.001000\n",
      "Step 400: loss=9.769238, lr=0.001000\n",
      "Step 500: loss=9.714314, lr=0.001000\n",
      "Step 600: loss=9.691904, lr=0.001000\n",
      "Step 700: loss=9.600872, lr=0.001000\n",
      "Step 800: loss=9.580305, lr=0.001000\n",
      "Step 900: loss=9.556435, lr=0.001000\n",
      "Step 1000: loss=9.534439, lr=0.001000\n",
      "Step 1100: loss=9.454253, lr=0.001000\n",
      "Step 1200: loss=9.418459, lr=0.001000\n",
      "Step 1300: loss=9.411724, lr=0.001000\n",
      "Step 1400: loss=9.350805, lr=0.001000\n",
      "Step 1500: loss=9.367863, lr=0.001000\n",
      "Step 1600: loss=9.343919, lr=0.001000\n",
      "Step 1700: loss=9.370528, lr=0.001000\n",
      "Step 1800: loss=9.299060, lr=0.001000\n",
      "Step 1900: loss=9.267596, lr=0.001000\n",
      "Step 2000: loss=9.286695, lr=0.001000\n",
      "Step 2100: loss=9.291334, lr=0.001000\n",
      "Step 2200: loss=9.250756, lr=0.001000\n",
      "Step 2300: loss=9.252250, lr=0.001000\n",
      "Step 2400: loss=9.266186, lr=0.001000\n",
      "Step 2500: loss=9.234799, lr=0.001000\n",
      "Step 2600: loss=9.209524, lr=0.001000\n",
      "Step 2700: loss=9.200559, lr=0.001000\n",
      "Step 2800: loss=9.217037, lr=0.001000\n",
      "Step 2900: loss=9.233882, lr=0.001000\n",
      "Step 3000: loss=9.171758, lr=0.001000\n",
      "Step 3100: loss=9.169518, lr=0.001000\n",
      "Step 3200: loss=9.098680, lr=0.001000\n",
      "Step 3300: loss=9.151955, lr=0.001000\n",
      "Step 3400: loss=9.188463, lr=0.001000\n",
      "Step 3500: loss=9.072914, lr=0.001000\n",
      "Step 3600: loss=9.166903, lr=0.001000\n",
      "Step 3700: loss=9.149531, lr=0.001000\n",
      "Step 3800: loss=9.091306, lr=0.001000\n",
      "Step 3900: loss=9.196313, lr=0.001000\n",
      "Step 4000: loss=9.126475, lr=0.001000\n",
      "Step 4100: loss=9.045840, lr=0.001000\n",
      "Step 4200: loss=9.122853, lr=0.001000\n",
      "Step 4300: loss=9.205229, lr=0.001000\n",
      "Step 4400: loss=9.061852, lr=0.001000\n",
      "Step 4500: loss=9.182652, lr=0.001000\n",
      "Step 4600: loss=9.017183, lr=0.001000\n",
      "Step 4700: loss=9.007379, lr=0.001000\n",
      "Step 4800: loss=9.065382, lr=0.001000\n",
      "Step 4900: loss=9.113276, lr=0.001000\n",
      "Step 4999: loss=9.025432, lr=0.001000\n",
      "\n",
      "=== Router Selection Statistics ===\n",
      "Total selections: 5000000\n",
      "Batch size: 1000\n",
      "Expert counts: {1: 950621, 0: 4049379}\n",
      "Expert percentages:\n",
      "  Expert 1: 19.01%\n",
      "  Expert 0: 80.99%\n",
      "Expected percentage per expert: 50.00%\n",
      "Load balancing variance: 960.23\n",
      "Step 0: loss=9.107011, lr=0.001000\n",
      "\n",
      "Step 50 - Router evolution:\n",
      "  Expert 0: 81.50%\n",
      "  Expert 1: 18.50%\n",
      "Step 50: loss=9.095901, lr=0.001000\n",
      "\n",
      "Step 100 - Router evolution:\n",
      "  Expert 0: 80.20%\n",
      "  Expert 1: 19.80%\n",
      "Step 100: loss=9.103376, lr=0.001000\n",
      "\n",
      "Step 150 - Router evolution:\n",
      "  Expert 0: 79.90%\n",
      "  Expert 1: 20.10%\n",
      "Step 150: loss=8.991455, lr=0.001000\n",
      "\n",
      "Step 200 - Router evolution:\n",
      "  Expert 1: 19.20%\n",
      "  Expert 0: 80.80%\n",
      "Step 200: loss=9.057537, lr=0.001000\n",
      "\n",
      "Step 250 - Router evolution:\n",
      "  Expert 0: 80.90%\n",
      "  Expert 1: 19.10%\n",
      "Step 250: loss=9.041873, lr=0.001000\n",
      "\n",
      "Step 300 - Router evolution:\n",
      "  Expert 0: 81.10%\n",
      "  Expert 1: 18.90%\n",
      "Step 300: loss=9.047122, lr=0.001000\n",
      "\n",
      "Step 350 - Router evolution:\n",
      "  Expert 0: 80.70%\n",
      "  Expert 1: 19.30%\n",
      "Step 350: loss=9.073485, lr=0.001000\n",
      "\n",
      "Step 400 - Router evolution:\n",
      "  Expert 0: 80.90%\n",
      "  Expert 1: 19.10%\n",
      "Step 400: loss=9.069619, lr=0.001000\n",
      "\n",
      "Step 450 - Router evolution:\n",
      "  Expert 1: 17.90%\n",
      "  Expert 0: 82.10%\n",
      "Step 450: loss=9.038252, lr=0.001000\n",
      "\n",
      "Step 500 - Router evolution:\n",
      "  Expert 0: 79.80%\n",
      "  Expert 1: 20.20%\n",
      "Step 500: loss=9.034648, lr=0.001000\n",
      "\n",
      "Step 550 - Router evolution:\n",
      "  Expert 1: 20.30%\n",
      "  Expert 0: 79.70%\n",
      "Step 550: loss=9.133092, lr=0.001000\n",
      "\n",
      "Step 600 - Router evolution:\n",
      "  Expert 0: 81.60%\n",
      "  Expert 1: 18.40%\n",
      "Step 600: loss=8.987126, lr=0.001000\n",
      "\n",
      "Step 650 - Router evolution:\n",
      "  Expert 1: 19.00%\n",
      "  Expert 0: 81.00%\n",
      "Step 650: loss=9.021470, lr=0.001000\n",
      "\n",
      "Step 700 - Router evolution:\n",
      "  Expert 1: 20.60%\n",
      "  Expert 0: 79.40%\n",
      "Step 700: loss=8.954850, lr=0.001000\n",
      "\n",
      "Step 750 - Router evolution:\n",
      "  Expert 0: 80.30%\n",
      "  Expert 1: 19.70%\n",
      "Step 750: loss=9.042677, lr=0.001000\n",
      "\n",
      "Step 800 - Router evolution:\n",
      "  Expert 0: 81.50%\n",
      "  Expert 1: 18.50%\n",
      "Step 800: loss=9.007813, lr=0.001000\n",
      "\n",
      "Step 850 - Router evolution:\n",
      "  Expert 1: 18.90%\n",
      "  Expert 0: 81.10%\n",
      "Step 850: loss=8.947845, lr=0.001000\n",
      "\n",
      "Step 900 - Router evolution:\n",
      "  Expert 0: 81.40%\n",
      "  Expert 1: 18.60%\n",
      "Step 900: loss=9.043441, lr=0.001000\n",
      "\n",
      "Step 950 - Router evolution:\n",
      "  Expert 0: 82.10%\n",
      "  Expert 1: 17.90%\n",
      "Step 950: loss=9.012264, lr=0.001000\n",
      "\n",
      "Step 1000 - Router evolution:\n",
      "  Expert 0: 81.90%\n",
      "  Expert 1: 18.10%\n",
      "Step 1000: loss=8.986715, lr=0.001000\n",
      "\n",
      "Step 1050 - Router evolution:\n",
      "  Expert 0: 80.80%\n",
      "  Expert 1: 19.20%\n",
      "Step 1050: loss=9.007144, lr=0.001000\n",
      "\n",
      "Step 1100 - Router evolution:\n",
      "  Expert 0: 79.30%\n",
      "  Expert 1: 20.70%\n",
      "Step 1100: loss=8.928109, lr=0.001000\n",
      "\n",
      "Step 1150 - Router evolution:\n",
      "  Expert 0: 79.60%\n",
      "  Expert 1: 20.40%\n",
      "Step 1150: loss=8.916773, lr=0.001000\n",
      "\n",
      "Step 1200 - Router evolution:\n",
      "  Expert 0: 80.30%\n",
      "  Expert 1: 19.70%\n",
      "Step 1200: loss=9.025208, lr=0.001000\n",
      "\n",
      "Step 1250 - Router evolution:\n",
      "  Expert 0: 82.60%\n",
      "  Expert 1: 17.40%\n",
      "Step 1250: loss=9.044387, lr=0.001000\n",
      "\n",
      "Step 1300 - Router evolution:\n",
      "  Expert 1: 19.10%\n",
      "  Expert 0: 80.90%\n",
      "Step 1300: loss=9.026486, lr=0.001000\n",
      "\n",
      "Step 1350 - Router evolution:\n",
      "  Expert 0: 81.00%\n",
      "  Expert 1: 19.00%\n",
      "Step 1350: loss=8.980611, lr=0.001000\n",
      "\n",
      "Step 1400 - Router evolution:\n",
      "  Expert 0: 80.10%\n",
      "  Expert 1: 19.90%\n",
      "Step 1400: loss=9.060268, lr=0.001000\n",
      "\n",
      "Step 1450 - Router evolution:\n",
      "  Expert 0: 81.30%\n",
      "  Expert 1: 18.70%\n",
      "Step 1450: loss=8.912732, lr=0.001000\n",
      "\n",
      "Step 1500 - Router evolution:\n",
      "  Expert 0: 82.00%\n",
      "  Expert 1: 18.00%\n",
      "Step 1500: loss=9.027394, lr=0.001000\n",
      "\n",
      "Step 1550 - Router evolution:\n",
      "  Expert 0: 79.00%\n",
      "  Expert 1: 21.00%\n",
      "Step 1550: loss=8.989409, lr=0.001000\n",
      "\n",
      "Step 1600 - Router evolution:\n",
      "  Expert 0: 81.00%\n",
      "  Expert 1: 19.00%\n",
      "Step 1600: loss=8.993146, lr=0.001000\n",
      "\n",
      "Step 1650 - Router evolution:\n",
      "  Expert 0: 80.30%\n",
      "  Expert 1: 19.70%\n",
      "Step 1650: loss=8.937758, lr=0.001000\n",
      "\n",
      "Step 1700 - Router evolution:\n",
      "  Expert 0: 80.20%\n",
      "  Expert 1: 19.80%\n",
      "Step 1700: loss=8.976632, lr=0.001000\n",
      "\n",
      "Step 1750 - Router evolution:\n",
      "  Expert 1: 20.00%\n",
      "  Expert 0: 80.00%\n",
      "Step 1750: loss=8.941308, lr=0.001000\n",
      "\n",
      "Step 1800 - Router evolution:\n",
      "  Expert 0: 82.10%\n",
      "  Expert 1: 17.90%\n",
      "Step 1800: loss=8.943403, lr=0.001000\n",
      "\n",
      "Step 1850 - Router evolution:\n",
      "  Expert 0: 80.50%\n",
      "  Expert 1: 19.50%\n",
      "Step 1850: loss=9.049340, lr=0.001000\n",
      "\n",
      "Step 1900 - Router evolution:\n",
      "  Expert 0: 80.60%\n",
      "  Expert 1: 19.40%\n",
      "Step 1900: loss=8.927920, lr=0.001000\n",
      "\n",
      "Step 1950 - Router evolution:\n",
      "  Expert 0: 81.60%\n",
      "  Expert 1: 18.40%\n",
      "Step 1950: loss=8.966870, lr=0.001000\n",
      "\n",
      "Step 2000 - Router evolution:\n",
      "  Expert 0: 81.00%\n",
      "  Expert 1: 19.00%\n",
      "Step 2000: loss=8.973844, lr=0.001000\n",
      "\n",
      "Step 2050 - Router evolution:\n",
      "  Expert 0: 80.40%\n",
      "  Expert 1: 19.60%\n",
      "Step 2050: loss=9.061640, lr=0.001000\n",
      "\n",
      "Step 2100 - Router evolution:\n",
      "  Expert 1: 19.40%\n",
      "  Expert 0: 80.60%\n",
      "Step 2100: loss=8.994065, lr=0.001000\n",
      "\n",
      "Step 2150 - Router evolution:\n",
      "  Expert 0: 82.60%\n",
      "  Expert 1: 17.40%\n",
      "Step 2150: loss=9.083870, lr=0.001000\n",
      "\n",
      "Step 2200 - Router evolution:\n",
      "  Expert 0: 81.90%\n",
      "  Expert 1: 18.10%\n",
      "Step 2200: loss=8.953889, lr=0.001000\n",
      "\n",
      "Step 2250 - Router evolution:\n",
      "  Expert 0: 81.10%\n",
      "  Expert 1: 18.90%\n",
      "Step 2250: loss=8.940637, lr=0.001000\n",
      "\n",
      "Step 2300 - Router evolution:\n",
      "  Expert 0: 82.50%\n",
      "  Expert 1: 17.50%\n",
      "Step 2300: loss=9.065329, lr=0.001000\n",
      "\n",
      "Step 2350 - Router evolution:\n",
      "  Expert 0: 81.00%\n",
      "  Expert 1: 19.00%\n",
      "Step 2350: loss=9.005734, lr=0.001000\n",
      "\n",
      "Step 2400 - Router evolution:\n",
      "  Expert 0: 81.50%\n",
      "  Expert 1: 18.50%\n",
      "Step 2400: loss=8.909188, lr=0.001000\n",
      "\n",
      "Step 2450 - Router evolution:\n",
      "  Expert 0: 79.70%\n",
      "  Expert 1: 20.30%\n",
      "Step 2450: loss=8.956537, lr=0.001000\n",
      "\n",
      "Step 2500 - Router evolution:\n",
      "  Expert 0: 80.40%\n",
      "  Expert 1: 19.60%\n",
      "Step 2500: loss=9.009894, lr=0.001000\n",
      "\n",
      "Step 2550 - Router evolution:\n",
      "  Expert 0: 82.70%\n",
      "  Expert 1: 17.30%\n",
      "Step 2550: loss=9.002055, lr=0.001000\n",
      "\n",
      "Step 2600 - Router evolution:\n",
      "  Expert 0: 81.90%\n",
      "  Expert 1: 18.10%\n",
      "Step 2600: loss=8.937366, lr=0.001000\n",
      "\n",
      "Step 2650 - Router evolution:\n",
      "  Expert 0: 79.40%\n",
      "  Expert 1: 20.60%\n",
      "Step 2650: loss=8.964047, lr=0.001000\n",
      "\n",
      "Step 2700 - Router evolution:\n",
      "  Expert 0: 80.40%\n",
      "  Expert 1: 19.60%\n",
      "Step 2700: loss=9.054272, lr=0.001000\n",
      "\n",
      "Step 2750 - Router evolution:\n",
      "  Expert 1: 19.10%\n",
      "  Expert 0: 80.90%\n",
      "Step 2750: loss=9.017105, lr=0.001000\n",
      "\n",
      "Step 2800 - Router evolution:\n",
      "  Expert 0: 78.70%\n",
      "  Expert 1: 21.30%\n",
      "Step 2800: loss=8.884691, lr=0.001000\n",
      "\n",
      "Step 2850 - Router evolution:\n",
      "  Expert 0: 82.50%\n",
      "  Expert 1: 17.50%\n",
      "Step 2850: loss=8.965734, lr=0.001000\n",
      "\n",
      "Step 2900 - Router evolution:\n",
      "  Expert 0: 80.30%\n",
      "  Expert 1: 19.70%\n",
      "Step 2900: loss=9.005765, lr=0.001000\n",
      "\n",
      "Step 2950 - Router evolution:\n",
      "  Expert 1: 19.90%\n",
      "  Expert 0: 80.10%\n",
      "Step 2950: loss=8.964420, lr=0.001000\n",
      "\n",
      "Step 3000 - Router evolution:\n",
      "  Expert 0: 81.40%\n",
      "  Expert 1: 18.60%\n",
      "Step 3000: loss=9.019023, lr=0.001000\n",
      "\n",
      "Step 3050 - Router evolution:\n",
      "  Expert 0: 80.40%\n",
      "  Expert 1: 19.60%\n",
      "Step 3050: loss=9.007256, lr=0.001000\n",
      "\n",
      "Step 3100 - Router evolution:\n",
      "  Expert 0: 78.30%\n",
      "  Expert 1: 21.70%\n",
      "Step 3100: loss=9.021386, lr=0.001000\n",
      "\n",
      "Step 3150 - Router evolution:\n",
      "  Expert 1: 19.20%\n",
      "  Expert 0: 80.80%\n",
      "Step 3150: loss=8.870263, lr=0.001000\n",
      "\n",
      "Step 3200 - Router evolution:\n",
      "  Expert 0: 81.40%\n",
      "  Expert 1: 18.60%\n",
      "Step 3200: loss=8.955042, lr=0.001000\n",
      "\n",
      "Step 3250 - Router evolution:\n",
      "  Expert 0: 80.40%\n",
      "  Expert 1: 19.60%\n",
      "Step 3250: loss=8.950912, lr=0.001000\n",
      "\n",
      "Step 3300 - Router evolution:\n",
      "  Expert 0: 79.40%\n",
      "  Expert 1: 20.60%\n",
      "Step 3300: loss=8.892801, lr=0.001000\n",
      "\n",
      "Step 3350 - Router evolution:\n",
      "  Expert 0: 82.20%\n",
      "  Expert 1: 17.80%\n",
      "Step 3350: loss=8.934099, lr=0.001000\n",
      "\n",
      "Step 3400 - Router evolution:\n",
      "  Expert 1: 18.60%\n",
      "  Expert 0: 81.40%\n",
      "Step 3400: loss=9.024320, lr=0.001000\n",
      "\n",
      "Step 3450 - Router evolution:\n",
      "  Expert 0: 80.10%\n",
      "  Expert 1: 19.90%\n",
      "Step 3450: loss=9.006036, lr=0.001000\n",
      "\n",
      "Step 3500 - Router evolution:\n",
      "  Expert 1: 19.80%\n",
      "  Expert 0: 80.20%\n",
      "Step 3500: loss=9.044212, lr=0.001000\n",
      "\n",
      "Step 3550 - Router evolution:\n",
      "  Expert 0: 80.90%\n",
      "  Expert 1: 19.10%\n",
      "Step 3550: loss=8.989530, lr=0.001000\n",
      "\n",
      "Step 3600 - Router evolution:\n",
      "  Expert 0: 82.70%\n",
      "  Expert 1: 17.30%\n",
      "Step 3600: loss=8.916424, lr=0.001000\n",
      "\n",
      "Step 3650 - Router evolution:\n",
      "  Expert 0: 80.60%\n",
      "  Expert 1: 19.40%\n",
      "Step 3650: loss=9.016269, lr=0.001000\n",
      "\n",
      "Step 3700 - Router evolution:\n",
      "  Expert 0: 82.40%\n",
      "  Expert 1: 17.60%\n",
      "Step 3700: loss=8.895691, lr=0.001000\n",
      "\n",
      "Step 3750 - Router evolution:\n",
      "  Expert 1: 18.10%\n",
      "  Expert 0: 81.90%\n",
      "Step 3750: loss=9.067356, lr=0.001000\n",
      "\n",
      "Step 3800 - Router evolution:\n",
      "  Expert 0: 82.20%\n",
      "  Expert 1: 17.80%\n",
      "Step 3800: loss=8.902716, lr=0.001000\n",
      "\n",
      "Step 3850 - Router evolution:\n",
      "  Expert 0: 78.20%\n",
      "  Expert 1: 21.80%\n",
      "Step 3850: loss=8.925277, lr=0.001000\n",
      "\n",
      "Step 3900 - Router evolution:\n",
      "  Expert 0: 80.70%\n",
      "  Expert 1: 19.30%\n",
      "Step 3900: loss=8.937129, lr=0.001000\n",
      "\n",
      "Step 3950 - Router evolution:\n",
      "  Expert 0: 80.80%\n",
      "  Expert 1: 19.20%\n",
      "Step 3950: loss=8.962534, lr=0.001000\n",
      "\n",
      "Step 4000 - Router evolution:\n",
      "  Expert 0: 80.50%\n",
      "  Expert 1: 19.50%\n",
      "Step 4000: loss=9.092843, lr=0.001000\n",
      "\n",
      "Step 4050 - Router evolution:\n",
      "  Expert 1: 19.70%\n",
      "  Expert 0: 80.30%\n",
      "Step 4050: loss=8.982697, lr=0.001000\n",
      "\n",
      "Step 4100 - Router evolution:\n",
      "  Expert 0: 82.00%\n",
      "  Expert 1: 18.00%\n",
      "Step 4100: loss=8.936410, lr=0.001000\n",
      "\n",
      "Step 4150 - Router evolution:\n",
      "  Expert 0: 80.50%\n",
      "  Expert 1: 19.50%\n",
      "Step 4150: loss=9.066889, lr=0.001000\n",
      "\n",
      "Step 4200 - Router evolution:\n",
      "  Expert 0: 81.30%\n",
      "  Expert 1: 18.70%\n",
      "Step 4200: loss=8.973495, lr=0.001000\n",
      "\n",
      "Step 4250 - Router evolution:\n",
      "  Expert 0: 81.80%\n",
      "  Expert 1: 18.20%\n",
      "Step 4250: loss=9.017582, lr=0.001000\n",
      "\n",
      "Step 4300 - Router evolution:\n",
      "  Expert 1: 16.80%\n",
      "  Expert 0: 83.20%\n",
      "Step 4300: loss=8.993022, lr=0.001000\n",
      "\n",
      "Step 4350 - Router evolution:\n",
      "  Expert 0: 82.40%\n",
      "  Expert 1: 17.60%\n",
      "Step 4350: loss=8.957741, lr=0.001000\n",
      "\n",
      "Step 4400 - Router evolution:\n",
      "  Expert 1: 19.80%\n",
      "  Expert 0: 80.20%\n",
      "Step 4400: loss=8.990954, lr=0.001000\n",
      "\n",
      "Step 4450 - Router evolution:\n",
      "  Expert 0: 79.60%\n",
      "  Expert 1: 20.40%\n",
      "Step 4450: loss=8.884772, lr=0.001000\n",
      "\n",
      "Step 4500 - Router evolution:\n",
      "  Expert 0: 79.40%\n",
      "  Expert 1: 20.60%\n",
      "Step 4500: loss=8.952317, lr=0.001000\n",
      "\n",
      "Step 4550 - Router evolution:\n",
      "  Expert 0: 81.60%\n",
      "  Expert 1: 18.40%\n",
      "Step 4550: loss=8.898336, lr=0.001000\n",
      "\n",
      "Step 4600 - Router evolution:\n",
      "  Expert 0: 81.70%\n",
      "  Expert 1: 18.30%\n",
      "Step 4600: loss=8.988692, lr=0.001000\n",
      "\n",
      "Step 4650 - Router evolution:\n",
      "  Expert 1: 18.20%\n",
      "  Expert 0: 81.80%\n",
      "Step 4650: loss=8.908232, lr=0.001000\n",
      "\n",
      "Step 4700 - Router evolution:\n",
      "  Expert 1: 20.10%\n",
      "  Expert 0: 79.90%\n",
      "Step 4700: loss=8.887957, lr=0.001000\n",
      "\n",
      "Step 4750 - Router evolution:\n",
      "  Expert 0: 81.10%\n",
      "  Expert 1: 18.90%\n",
      "Step 4750: loss=8.977669, lr=0.001000\n",
      "\n",
      "Step 4800 - Router evolution:\n",
      "  Expert 0: 84.20%\n",
      "  Expert 1: 15.80%\n",
      "Step 4800: loss=8.894324, lr=0.001000\n",
      "\n",
      "Step 4850 - Router evolution:\n",
      "  Expert 0: 81.40%\n",
      "  Expert 1: 18.60%\n",
      "Step 4850: loss=8.915108, lr=0.001000\n",
      "\n",
      "Step 4900 - Router evolution:\n",
      "  Expert 0: 80.50%\n",
      "  Expert 1: 19.50%\n",
      "Step 4900: loss=9.036879, lr=0.001000\n",
      "\n",
      "Step 4950 - Router evolution:\n",
      "  Expert 0: 81.70%\n",
      "  Expert 1: 18.30%\n",
      "Step 4950: loss=8.926359, lr=0.001000\n",
      "\n",
      "Step 5000 - Router evolution:\n",
      "  Expert 0: 81.50%\n",
      "  Expert 1: 18.50%\n",
      "Step 4999: loss=8.986189, lr=0.001000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tools to track gate expert selection statistics\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class RouterSelectionHook:\n",
    "    def __init__(self):\n",
    "        self.expert_selections = {}  # {expert_id: count}\n",
    "        self.total_selections = 0\n",
    "        self.batch_size = 0\n",
    "        \n",
    "    def __call__(self, hook_data):\n",
    "        \"\"\"Hook function called during training.\"\"\"\n",
    "        model = hook_data['model']\n",
    "        batch = hook_data.get('batch', None)\n",
    "        \n",
    "        if batch is None:\n",
    "            # Generate a batch to analyze router behavior\n",
    "            batch = model.generate_batch(1000)  # Use large batch for good statistics\n",
    "        \n",
    "        # Get router probabilities for this batch\n",
    "        with torch.no_grad():\n",
    "            expert_weights, top_k_indices, _ = model.compute_active_experts(batch)\n",
    "            \n",
    "            # For k=1, track which expert was selected for each input\n",
    "            if model.config.n_active_experts == 1:\n",
    "                selected_experts = top_k_indices.squeeze(-1)  # [batch_size]\n",
    "                \n",
    "                # Count selections for each expert\n",
    "                for expert_id in selected_experts.cpu().numpy():\n",
    "                    if expert_id not in self.expert_selections:\n",
    "                        self.expert_selections[expert_id] = 0\n",
    "                    self.expert_selections[expert_id] += 1\n",
    "                \n",
    "                self.total_selections += len(selected_experts)\n",
    "                self.batch_size = len(selected_experts)\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get router selection statistics.\"\"\"\n",
    "        if self.total_selections == 0:\n",
    "            return {}\n",
    "        \n",
    "        stats = {\n",
    "            'total_selections': self.total_selections,\n",
    "            'expert_counts': dict(self.expert_selections),\n",
    "            'expert_percentages': {}\n",
    "        }\n",
    "        \n",
    "        for expert_id, count in self.expert_selections.items():\n",
    "            stats['expert_percentages'][expert_id] = (count / self.total_selections) * 100\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        \"\"\"Print current router selection statistics.\"\"\"\n",
    "        stats = self.get_statistics()\n",
    "        \n",
    "        if not stats:\n",
    "            print(\"No router selections recorded yet.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n=== Router Selection Statistics ===\")\n",
    "        print(f\"Total selections: {stats['total_selections']}\")\n",
    "        print(f\"Batch size: {self.batch_size}\")\n",
    "        print(f\"Expert counts: {stats['expert_counts']}\")\n",
    "        print(f\"Expert percentages:\")\n",
    "        for expert_id, percentage in stats['expert_percentages'].items():\n",
    "            print(f\"  Expert {expert_id}: {percentage:.2f}%\")\n",
    "        \n",
    "        # Check for load balancing\n",
    "        n_experts = len(stats['expert_counts'])\n",
    "        if n_experts > 0:\n",
    "            expected_percentage = 100.0 / n_experts\n",
    "            print(f\"Expected percentage per expert: {expected_percentage:.2f}%\")\n",
    "            \n",
    "            # Calculate load balancing metric\n",
    "            percentages = list(stats['expert_percentages'].values())\n",
    "            variance = np.var(percentages)\n",
    "            print(f\"Load balancing variance: {variance:.2f}\")\n",
    "\n",
    "# Usage example:\n",
    "def train_with_router_hook():\n",
    "    \"\"\"Train model with router selection tracking.\"\"\"\n",
    "    config = Config(\n",
    "        n_features=2,\n",
    "        n_hidden=1,\n",
    "        n_experts=2,\n",
    "        n_active_experts=1,\n",
    "        load_balancing_loss=True,\n",
    "    )\n",
    "    \n",
    "    model = MoEModel(\n",
    "        config=config,\n",
    "        device=DEVICE,\n",
    "        importance=torch.tensor([1, 1]),\n",
    "        feature_probability=torch.tensor(0.1)\n",
    "    )\n",
    "    \n",
    "    # Create router hook\n",
    "    router_hook = RouterSelectionHook()\n",
    "    \n",
    "    # Train with hook\n",
    "    optimize(model, n_batch=512, steps=5000, print_freq=100, lr=1e-3, hooks=[router_hook])\n",
    "    \n",
    "    # Print statistics\n",
    "    router_hook.print_statistics()\n",
    "    \n",
    "    return model, router_hook\n",
    "\n",
    "# Alternative: Hook that tracks router behavior over time\n",
    "class RouterEvolutionHook:\n",
    "    def __init__(self, log_interval=100):\n",
    "        self.log_interval = log_interval\n",
    "        self.step = 0\n",
    "        self.router_history = []  # List of (step, expert_percentages) tuples\n",
    "        \n",
    "    def __call__(self, hook_data):\n",
    "        self.step += 1\n",
    "        \n",
    "        if self.step % self.log_interval == 0:\n",
    "            # Create temporary hook to get current statistics\n",
    "            temp_hook = RouterSelectionHook()\n",
    "            temp_hook(hook_data)\n",
    "            \n",
    "            stats = temp_hook.get_statistics()\n",
    "            if stats:\n",
    "                self.router_history.append((self.step, stats['expert_percentages']))\n",
    "                \n",
    "                print(f\"\\nStep {self.step} - Router evolution:\")\n",
    "                for expert_id, percentage in stats['expert_percentages'].items():\n",
    "                    print(f\"  Expert {expert_id}: {percentage:.2f}%\")\n",
    "    \n",
    "    def plot_evolution(self):\n",
    "        \"\"\"Plot router evolution over time.\"\"\"\n",
    "        if not self.router_history:\n",
    "            print(\"No router history to plot.\")\n",
    "            return\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        steps = [step for step, _ in self.router_history]\n",
    "        expert_ids = set()\n",
    "        for _, percentages in self.router_history:\n",
    "            expert_ids.update(percentages.keys())\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for expert_id in sorted(expert_ids):\n",
    "            percentages = [data.get(expert_id, 0) for _, data in self.router_history]\n",
    "            plt.plot(steps, percentages, label=f'Expert {expert_id}', marker='o')\n",
    "        \n",
    "        plt.xlabel('Training Step')\n",
    "        plt.ylabel('Router Selection Percentage')\n",
    "        plt.title('Router Evolution During Training')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Train with router tracking\n",
    "    model, router_hook = train_with_router_hook()\n",
    "    \n",
    "    # Or train with evolution tracking\n",
    "    evolution_hook = RouterEvolutionHook(log_interval=50)\n",
    "    optimize(model, n_batch=512, steps=5000, print_freq=50, lr=1e-3, hooks=[evolution_hook])\n",
    "    evolution_hook.plot_evolution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
